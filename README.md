# StochasticGradient_Algorithm
## Introduction: 
 Save my Stochastic Gradient optimization algorithm code <br><br>
 
 SG improved in three ways: <br>
 > Direction1-Noise Reduction Method <br>
 > Direction2-Variance Reduction Method (SG have residual, can't use fixed stepsize) <br>
 > Direction3-Second Order Method (escaping saddle point) <br>

## Algorithm listï¼š
>Basic-GradientDescentVariants:<br>
>> {Batch Gradient Descent } Algorithm: [BatchGradientDescent.py](https://github.com/YEN-GitHub/StochasticGradient_Algorithm/tree/master/Basic-GradientDescentVariants/BatchGradientDescent.py) <br>
>> {Stochastic Gradient Descent } Algorithm: [StochasticGradientDescent.py](https://github.com/YEN-GitHub/StochasticGradient_Algorithm/tree/master/Basic-GradientDescentVariants/StochasticGradientDescent.py) <br>
>> {Mini-Batch Gradient Descent } Algorithm: [Mini-Batch_GradientDescent.py](https://github.com/YEN-GitHub/StochasticGradient_Algorithm/tree/master/Basic-GradientDescentVariants/Mini-Batch_GradientDescent.py) <br>

> Direction1-Noise Reduction Method: <br>
>> { } Algorithm: [.py](https://github.com/YEN-GitHub/StochasticGradient_Algorithm/tree/master/Direction1-NoiseReductionMethod/WolfeCondition.py) <br>

> Direction2-Variance Reduction Method:<br>
>> { } Algorithm: [.py](https://github.com/YEN-GitHub/StochasticGradient_Algorithm/tree/master/Direction2-VarianceReductionMethod/WolfeCondition.py) <br>

> Direction3-Second Order Method:<br>
>> { } Algorithm: [.py](https://github.com/YEN-GitHub/StochasticGradient_Algorithm/tree/master/Direction3-SecondOrderMethod/WolfeCondition.py) <br>

## References
> Paper:  `Optimization Methods for Large-Scale Machine Learning`  <br>
> Paper:  `An Overview of Gradient Descent Optimization Algorithms`  <br>
> Github user: `summersunshine1/optimize`   <br>
> Github user: `tsycnh/mlbasic`  <br>